from pyspark.sql import Window
import pyspark.sql.functions as F

# Step 1: Dynamically identify the amount columns (assuming non-ID, non-date columns are amounts)
amount_columns = [col for col in df.columns if col not in ['id', 'date']]

# Step 2: Define the window specification to partition by ID and order by date
window_spec = Window.partitionBy('id').orderBy('date')

# Step 3: Sum all the amount columns dynamically to create total_amt
df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

# Step 4: Function to adjust the negative amounts from bottom to top (roll up)
def adjust_negatives_up(df, amount_columns):
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Shift the amounts of previous row upwards using lag function for each amount column dynamically
        for amt in amount_columns:
            df = df.withColumn(f'prev_{amt}', F.lag(amt, 1).over(window_spec))
            
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            df = df.withColumn(amt, F.when(F.col('total_amt') < 0, F.col(amt) + F.col(f'prev_{amt}')).otherwise(F.col(amt)))
        
        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Remove columns used for shifting values
    return df.drop(*[f'prev_{amt}' for amt in amount_columns])

# Step 5: Function to adjust negative amounts from top to bottom (roll down)
def adjust_negatives_down(df, amount_columns):
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Shift the amounts of next row downwards using lead function for each amount column dynamically
        for amt in amount_columns:
            df = df.withColumn(f'next_{amt}', F.lead(amt, 1).over(window_spec))
            
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            df = df.withColumn(amt, F.when(F.col('total_amt') < 0, F.col(amt) + F.col(f'next_{amt}')).otherwise(F.col(amt)))
        
        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Remove columns used for shifting values
    return df.drop(*[f'next_{amt}' for amt in amount_columns])

# Step 6: Apply the bottom-to-top adjustment, followed by the top-to-bottom adjustment
df = adjust_negatives_up(df, amount_columns)
df = adjust_negatives_down(df, amount_columns)

# Step 7: After rolling up and down, set any remaining negative total to zero
df = df.withColumn('total_amt', F.when(F.col('total_amt') < 0, 0).otherwise(F.col('total_amt')))

# Step 8: Remove rows where total_amt is zero after adjustment
df = df.filter(F.col('total_amt') != 0)

# Step 9: Show the final dataframe
df.show()




#adjustment func


from pyspark.sql import Window
import pyspark.sql.functions as F

def adjust_negatives_up(df, amount_columns):
    window_spec = Window.partitionBy('id').orderBy('date')
    
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            # Add negative amounts to the previous row's amount
            df = df.withColumn(
                amt,
                F.when(F.col('total_amt') < 0, F.col(amt) + F.lag(amt, 1).over(window_spec)).otherwise(F.col(amt))
            )

        # Remove rows where total_amt is negative
        df = df.filter(F.col('total_amt') >= 0)

        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Final adjustment to ensure no negative total_amt and remove any negative rows
    df = df.filter(F.col('total_amt') >= 0)

    # Drop any rows with negative amounts, since those are now invalid
    for amt in amount_columns:
        df = df.filter(F.col(amt) >= 0)

    return df



#simentanoeus adjustment 

from pyspark.sql import SparkSession, Window
import pyspark.sql.functions as F

# Create a Spark session
spark = SparkSession.builder.appName("ModifyCurrentAndLead").getOrCreate()

# Sample data
data = [
    (1, "Day 1", 10),    # Current positive
    (1, "Day 2", -5),    # Current negative
    (1, "Day 3", 8),     # Current positive
    (1, "Day 4", -10),   # Current negative
    (1, "Day 5", 15)     # Current positive
]

# Create DataFrame
columns = ["ID", "Date", "amt"]
df = spark.createDataFrame(data, schema=columns)

# Define window specification to access current and next rows
window_spec = Window.partitionBy("ID").orderBy("Date")

# Show initial DataFrame
print("Initial DataFrame:")
df.show()

# Adjust both current row and lead row values simultaneously
df_adjusted = df.withColumn(
    "lead_amt",
    F.lead("amt").over(window_spec)  # Get the lead amount
).withColumn(
    "new_amt",
    F.when(F.col("amt") < 0, F.col("amt") + F.col("lead_amt"))  # Adjust current amount if negative
     .otherwise(F.col("amt"))  # Keep current amount if positive
).withColumn(
    "lead_amt",
    F.when(F.col("lead_amt") + F.col("amt") < 0, 0)  # Set lead to 0 if the adjustment would be negative
     .otherwise(F.col("lead_amt"))  # Keep lead amount if not negative
)

# Select the final adjusted columns
df_final = df_adjusted.select(
    "ID", 
    "Date", 
    "new_amt", 
    "lead_amt"
)

# Show the final adjusted DataFrame
print("Final Adjusted DataFrame:")
df_final.show()



#without create cols


from pyspark.sql import functions as F

for ant in amount_columns:
    df = df.withColumn(
        ant,
        F.when(
            F.lead("total_ent", offset=1).over(window_spec) < 8,
            F.round(
                F.when(
                    F.lead("total_ent", offset=1).over(window_spec) < 0,
                    F.lead(ant, offset=1).over(window_spec)
                ).otherwise(F.col(ant)), scale=2
            )
        ).otherwise(
            F.when(
                F.lag(ant, offset=1).over(window_spec).isNotNull(),
                F.lag(ant, offset=1).over(window_spec)
            ).otherwise(F.col(ant))
        )
    )


#simenteneous cols update using struct 

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define window spec and conditions
window_spec = Window.partitionBy('id').orderBy('date')
lag_col3 = F.lag('col3', 1).over(window_spec)

# Example conditions (You can modify them as per your logic)
cond1 = F.col('col3') > lag_col3
cond2 = F.col('col2') > 50
cond3 = F.col('col1') < 100

# Update col1 based on cond1 and cond2, else use another logic for col1
# Update col2 based on cond1 and cond3, else use another logic for col2
df_updated = df.withColumn(
    "combined",
    F.when(cond1 & cond2, F.struct((F.col('col1') * 2).alias('col1_updated'),
                                   F.col('col2').alias('col2_updated'))) # Logic for col1 when cond1 and cond2 are true
    .when(cond1 & cond3, F.struct(F.col('col1').alias('col1_updated'),
                                  (F.col('col2') + 20).alias('col2_updated'))) # Logic for col2 when cond1 and cond3 are true
    .otherwise(F.struct((F.col('col1') * 1.5).alias('col1_updated'),  # Default logic for col1
                        (F.col('col2') + 5).alias('col2_updated')))  # Default logic for col2
)

# Now extract col1 and col2 back from the 'combined' column
df_final = df_updated.withColumn('col1', F.col('combined.col1_updated')) \
                     .withColumn('col2', F.col('combined.col2_updated')) \
                     .drop('combined')

df_final.show()


#--------------------------------#
#df while approach without temp_df



from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define the window specification
window_spec_desc = Window.partitionBy('federalId').orderBy(F.desc('A07_LIAB_DATE'))

# While there are rows where LIAB_TOTAL is less than 0.00, continue processing
while df.filter(F.col("LIAB_TOTAL") < 0.00).count() != 0:
    # Print the count of rows where LIAB_TOTAL is less than 0.00
    print(df.filter(F.col("LIAB_TOTAL") < 0.00).count())
    
    # Calculate lagged values and row numbers for current iteration
    lag = F.lag(F.col("LIAB_TOTAL"), offset=1).over(window_spec_desc)
    row_number = F.row_number().over(window_spec_desc)
    
    # Get the maximum row number in the current partition
    max_row_number = df.select(F.max(row_number)).first()[0]

    # Check if there are any negative values in the last position of each partition
    last_neg_condition = df.filter((F.col("LIAB_TOTAL") < 0.00) & (row_number == max_row_number)).count() > 0
    
    # Define conditions
    cond1 = (lag.isNotNull() & (lag < 0.00))
    cond2 = (F.col("LIAB_TOTAL") < 0.00)

    # Update the 'LIAB_TOTAL' column based on conditions
    df = df.withColumn("LIAB_TOTAL", 
        F.when(last_neg_condition, F.col("LIAB_TOTAL"))  # If last value is negative, keep it as is
        .otherwise(F.when(cond1, F.round(lag + F.col("LIAB_TOTAL"), 2))  # If cond1 is true, sum lag and now
        .otherwise(F.when(cond2, F.lit(0.00))  # If cond2 is true, set to 0.00
        .otherwise(F.col("LIAB_TOTAL")))
    )

# If necessary, show the final DataFrame
df.show()

#---------------------------#
#with temp_df

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define the window specification
window_spec_desc = Window.partitionBy('federalId').orderBy(F.desc('A07_LIAB_DATE'))

# Calculate lagged values, current values, and row numbers in a separate DataFrame
temp_df = df.withColumn("lag", F.lag(F.col("LIAB_TOTAL"), offset=1).over(window_spec_desc)) \
             .withColumn("row_number", F.row_number().over(window_spec_desc))

# While there are rows where LIAB_TOTAL is less than 0.00, continue processing
while temp_df.filter(F.col("LIAB_TOTAL") < 0.00).count() != 0:
    # Print the count of rows where LIAB_TOTAL is less than 0.00
    print(temp_df.filter(F.col("LIAB_TOTAL") < 0.00).count())
    
    # Determine the maximum row number in the current partition
    max_row_number = temp_df.agg(F.max("row_number")).first()[0]
    
    # Check if there are any negative values in the last position of each partition
    last_neg_condition = temp_df.filter((F.col("LIAB_TOTAL") < 0.00) & (F.col("row_number") == max_row_number)).count() > 0
    
    # Define conditions
    cond1 = (F.col("lag").isNotNull() & (F.col("lag") < 0.00))
    cond2 = (F.col("LIAB_TOTAL") < 0.00)

    # Update the 'LIAB_TOTAL' column based on conditions
    df = df.withColumn("LIAB_TOTAL", 
        F.when(last_neg_condition, F.col("LIAB_TOTAL"))  # If last value is negative, keep it as is
        .otherwise(F.when(cond1, F.round(F.col("lag") + F.col("LIAB_TOTAL"), 2))  # If cond1 is true, sum lag and now
        .otherwise(F.when(cond2, F.lit(0.00))  # If cond2 is true, set to 0.00
        .otherwise(F.col("LIAB_TOTAL")))
    )
    
    # Recalculate the temporary DataFrame
    temp_df = df.withColumn("lag", F.lag(F.col("LIAB_TOTAL"), offset=1).over(window_spec_desc)) \
                 .withColumn("row_number", F.row_number().over(window_spec_desc))


#force balancing 

from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Assuming df is the DataFrame you're working with

# Step 1: Calculate the difference between Amount and IdTotal
df = df.withColumn("diff", F.col("IdTotal") - F.col("Amount"))

# Step 2: Filter where there's a mismatch
mismatched_df = df.filter(F.col("diff") != 0)

# Step 3: Check if the given date (240430) is present for the same Id
windowSpec = Window.partitionBy("Id").orderBy("Date")

# Get the row corresponding to the given date
date_exists = mismatched_df.filter(F.col("Date") == 240430)

# If the date exists, update the Amount by adding the difference
updated_date_exists = date_exists.withColumn("Amount", F.col("Amount") + F.col("diff"))

# If the date does not exist, create a new row with the given date and set Amount to the diff value
new_row_df = mismatched_df.filter(~(F.col("Date") == 240430)) \
    .withColumn("new_date", F.lit(240430)) \
    .withColumn("Amount", F.col("diff")) \
    .select("Id", "new_date", "Amount", "IdTotal")

# Union the two DataFrames to get the final output
final_df = updated_date_exists.union(new_row_df).select("Id", "Date", "Amount", "IdTotal")

# Display the final DataFrame
final_df.show()