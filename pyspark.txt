from pyspark.sql import Window
import pyspark.sql.functions as F

# Step 1: Dynamically identify the amount columns (assuming non-ID, non-date columns are amounts)
amount_columns = [col for col in df.columns if col not in ['id', 'date']]

# Step 2: Define the window specification to partition by ID and order by date
window_spec = Window.partitionBy('id').orderBy('date')

# Step 3: Sum all the amount columns dynamically to create total_amt
df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

# Step 4: Function to adjust the negative amounts from bottom to top (roll up)
def adjust_negatives_up(df, amount_columns):
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Shift the amounts of previous row upwards using lag function for each amount column dynamically
        for amt in amount_columns:
            df = df.withColumn(f'prev_{amt}', F.lag(amt, 1).over(window_spec))
            
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            df = df.withColumn(amt, F.when(F.col('total_amt') < 0, F.col(amt) + F.col(f'prev_{amt}')).otherwise(F.col(amt)))
        
        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Remove columns used for shifting values
    return df.drop(*[f'prev_{amt}' for amt in amount_columns])

# Step 5: Function to adjust negative amounts from top to bottom (roll down)
def adjust_negatives_down(df, amount_columns):
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Shift the amounts of next row downwards using lead function for each amount column dynamically
        for amt in amount_columns:
            df = df.withColumn(f'next_{amt}', F.lead(amt, 1).over(window_spec))
            
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            df = df.withColumn(amt, F.when(F.col('total_amt') < 0, F.col(amt) + F.col(f'next_{amt}')).otherwise(F.col(amt)))
        
        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Remove columns used for shifting values
    return df.drop(*[f'next_{amt}' for amt in amount_columns])

# Step 6: Apply the bottom-to-top adjustment, followed by the top-to-bottom adjustment
df = adjust_negatives_up(df, amount_columns)
df = adjust_negatives_down(df, amount_columns)

# Step 7: After rolling up and down, set any remaining negative total to zero
df = df.withColumn('total_amt', F.when(F.col('total_amt') < 0, 0).otherwise(F.col('total_amt')))

# Step 8: Remove rows where total_amt is zero after adjustment
df = df.filter(F.col('total_amt') != 0)

# Step 9: Show the final dataframe
df.show()




#adjustment func


from pyspark.sql import Window
import pyspark.sql.functions as F

def adjust_negatives_up(df, amount_columns):
    window_spec = Window.partitionBy('id').orderBy('date')
    
    while True:
        # Identify rows where total amount is negative
        negative_rows = df.filter(F.col('total_amt') < 0)
        
        # If no negative rows remain, stop the loop
        if negative_rows.count() == 0:
            break
        
        # Adjust negative amounts dynamically
        for amt in amount_columns:
            # Add negative amounts to the previous row's amount
            df = df.withColumn(
                amt,
                F.when(F.col('total_amt') < 0, F.col(amt) + F.lag(amt, 1).over(window_spec)).otherwise(F.col(amt))
            )

        # Remove rows where total_amt is negative
        df = df.filter(F.col('total_amt') >= 0)

        # Recalculate total_amt after adjustment
        df = df.withColumn('total_amt', sum([F.col(col) for col in amount_columns]))

    # Final adjustment to ensure no negative total_amt and remove any negative rows
    df = df.filter(F.col('total_amt') >= 0)

    # Drop any rows with negative amounts, since those are now invalid
    for amt in amount_columns:
        df = df.filter(F.col(amt) >= 0)

    return df



#simentanoeus adjustment 

from pyspark.sql import SparkSession, Window
import pyspark.sql.functions as F

# Create a Spark session
spark = SparkSession.builder.appName("ModifyCurrentAndLead").getOrCreate()

# Sample data
data = [
    (1, "Day 1", 10),    # Current positive
    (1, "Day 2", -5),    # Current negative
    (1, "Day 3", 8),     # Current positive
    (1, "Day 4", -10),   # Current negative
    (1, "Day 5", 15)     # Current positive
]

# Create DataFrame
columns = ["ID", "Date", "amt"]
df = spark.createDataFrame(data, schema=columns)

# Define window specification to access current and next rows
window_spec = Window.partitionBy("ID").orderBy("Date")

# Show initial DataFrame
print("Initial DataFrame:")
df.show()

# Adjust both current row and lead row values simultaneously
df_adjusted = df.withColumn(
    "lead_amt",
    F.lead("amt").over(window_spec)  # Get the lead amount
).withColumn(
    "new_amt",
    F.when(F.col("amt") < 0, F.col("amt") + F.col("lead_amt"))  # Adjust current amount if negative
     .otherwise(F.col("amt"))  # Keep current amount if positive
).withColumn(
    "lead_amt",
    F.when(F.col("lead_amt") + F.col("amt") < 0, 0)  # Set lead to 0 if the adjustment would be negative
     .otherwise(F.col("lead_amt"))  # Keep lead amount if not negative
)

# Select the final adjusted columns
df_final = df_adjusted.select(
    "ID", 
    "Date", 
    "new_amt", 
    "lead_amt"
)

# Show the final adjusted DataFrame
print("Final Adjusted DataFrame:")
df_final.show()



#without create cols


from pyspark.sql import functions as F

for ant in amount_columns:
    df = df.withColumn(
        ant,
        F.when(
            F.lead("total_ent", offset=1).over(window_spec) < 8,
            F.round(
                F.when(
                    F.lead("total_ent", offset=1).over(window_spec) < 0,
                    F.lead(ant, offset=1).over(window_spec)
                ).otherwise(F.col(ant)), scale=2
            )
        ).otherwise(
            F.when(
                F.lag(ant, offset=1).over(window_spec).isNotNull(),
                F.lag(ant, offset=1).over(window_spec)
            ).otherwise(F.col(ant))
        )
    )